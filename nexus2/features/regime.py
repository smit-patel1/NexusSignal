"""
Regime Detection using Hidden Markov Models

Markets exhibit different regimes (bull/bear, high/low volatility)
with distinct statistical properties. HMMs can identify these
regimes from observable data.

Key insight: Returns are not IID - they're generated by a mixture
of processes (regimes). HMM captures this by:
1. Modeling latent states (regimes)
2. Estimating transition probabilities
3. Computing posterior regime probabilities

Reference: Hamilton (1989), "A New Approach to the Economic Analysis
of Nonstationary Time Series and the Business Cycle"
"""

from typing import Tuple, Optional, Dict, List
import numpy as np
import pandas as pd
from dataclasses import dataclass


@dataclass
class HMMResult:
    """Results from HMM fitting."""
    n_regimes: int
    means: np.ndarray              # Mean return per regime
    stds: np.ndarray               # Std per regime
    transition_matrix: np.ndarray  # Transition probabilities
    stationary_dist: np.ndarray    # Long-run regime probabilities
    regime_sequence: np.ndarray    # Most likely regime sequence (Viterbi)
    regime_probs: np.ndarray       # Posterior probabilities for each regime
    log_likelihood: float


def _forward_backward(
    observations: np.ndarray,
    initial_probs: np.ndarray,
    transition_matrix: np.ndarray,
    emission_probs: np.ndarray
) -> Tuple[np.ndarray, np.ndarray, float]:
    """
    Forward-backward algorithm for HMM inference.
    
    Args:
        observations: Observed sequence (not used directly, emission_probs computed externally)
        initial_probs: Initial state probabilities
        transition_matrix: State transition matrix
        emission_probs: P(observation | state) for each timestep and state
    
    Returns:
        Tuple of (alpha, beta, log_likelihood)
        alpha[t, k] = P(z_t = k, x_1:t)
        beta[t, k] = P(x_{t+1}:T | z_t = k)
    """
    T, K = emission_probs.shape
    
    # Forward pass
    alpha = np.zeros((T, K))
    scale = np.zeros(T)
    
    # Initialize
    alpha[0] = initial_probs * emission_probs[0]
    scale[0] = alpha[0].sum()
    alpha[0] /= scale[0]
    
    # Forward recursion
    for t in range(1, T):
        alpha[t] = (alpha[t-1] @ transition_matrix) * emission_probs[t]
        scale[t] = alpha[t].sum()
        if scale[t] > 0:
            alpha[t] /= scale[t]
    
    # Backward pass
    beta = np.zeros((T, K))
    beta[-1] = 1.0
    
    for t in range(T-2, -1, -1):
        beta[t] = transition_matrix @ (emission_probs[t+1] * beta[t+1])
        if scale[t+1] > 0:
            beta[t] /= scale[t+1]
    
    log_likelihood = np.sum(np.log(scale + 1e-10))
    
    return alpha, beta, log_likelihood


def _viterbi(
    initial_probs: np.ndarray,
    transition_matrix: np.ndarray,
    emission_probs: np.ndarray
) -> np.ndarray:
    """
    Viterbi algorithm for most likely state sequence.
    
    Returns:
        Most probable state sequence
    """
    T, K = emission_probs.shape
    
    # Log probabilities for numerical stability
    log_trans = np.log(transition_matrix + 1e-10)
    log_emit = np.log(emission_probs + 1e-10)
    log_init = np.log(initial_probs + 1e-10)
    
    # DP tables
    V = np.zeros((T, K))
    backpointer = np.zeros((T, K), dtype=int)
    
    # Initialize
    V[0] = log_init + log_emit[0]
    
    # Forward pass
    for t in range(1, T):
        for k in range(K):
            probs = V[t-1] + log_trans[:, k]
            backpointer[t, k] = np.argmax(probs)
            V[t, k] = probs[backpointer[t, k]] + log_emit[t, k]
    
    # Backtrace
    states = np.zeros(T, dtype=int)
    states[-1] = np.argmax(V[-1])
    
    for t in range(T-2, -1, -1):
        states[t] = backpointer[t+1, states[t+1]]
    
    return states


def _gaussian_emission(
    x: np.ndarray,
    means: np.ndarray,
    stds: np.ndarray
) -> np.ndarray:
    """
    Compute Gaussian emission probabilities.
    
    Args:
        x: Observations (T,)
        means: Mean per regime (K,)
        stds: Std per regime (K,)
    
    Returns:
        Emission probabilities (T, K)
    """
    T = len(x)
    K = len(means)
    
    emit = np.zeros((T, K))
    
    for k in range(K):
        # Gaussian PDF
        emit[:, k] = np.exp(-0.5 * ((x - means[k]) / stds[k])**2) / (stds[k] * np.sqrt(2 * np.pi))
    
    # Normalize
    emit = emit / (emit.sum(axis=1, keepdims=True) + 1e-10)
    
    return emit


def fit_hmm(
    returns: np.ndarray,
    n_regimes: int = 3,
    n_iter: int = 100,
    tol: float = 1e-4,
    random_state: int = 42
) -> HMMResult:
    """
    Fit Gaussian HMM to returns using Baum-Welch algorithm.
    
    The model assumes returns are generated by a mixture of
    Gaussian distributions, with regime transitions following
    a Markov chain.
    
    Args:
        returns: Return series
        n_regimes: Number of hidden states
        n_iter: Maximum EM iterations
        tol: Convergence tolerance
        random_state: Random seed
    
    Returns:
        HMMResult with fitted parameters
        
    Typical interpretations for 3 regimes:
    - Regime 0: Low volatility bull market
    - Regime 1: Normal market
    - Regime 2: High volatility / crisis
    """
    np.random.seed(random_state)
    
    T = len(returns)
    K = n_regimes
    
    # Initialize parameters using k-means style clustering
    sorted_returns = np.sort(returns)
    quantiles = [sorted_returns[int(T * (i + 0.5) / K)] for i in range(K)]
    means = np.array(quantiles)
    stds = np.full(K, returns.std())
    
    # Initialize transition matrix (slightly favor staying in same state)
    transition_matrix = np.full((K, K), 0.1 / (K - 1))
    np.fill_diagonal(transition_matrix, 0.9)
    
    # Initial state distribution (uniform)
    initial_probs = np.ones(K) / K
    
    prev_ll = -np.inf
    
    for iteration in range(n_iter):
        # E-step: Compute responsibilities
        emission_probs = _gaussian_emission(returns, means, stds)
        alpha, beta, log_likelihood = _forward_backward(
            returns, initial_probs, transition_matrix, emission_probs
        )
        
        # Check convergence
        if abs(log_likelihood - prev_ll) < tol:
            break
        prev_ll = log_likelihood
        
        # Posterior probabilities (gamma)
        gamma = alpha * beta
        gamma = gamma / (gamma.sum(axis=1, keepdims=True) + 1e-10)
        
        # Xi (transition posteriors)
        xi = np.zeros((T-1, K, K))
        for t in range(T-1):
            for i in range(K):
                for j in range(K):
                    xi[t, i, j] = alpha[t, i] * transition_matrix[i, j] * emission_probs[t+1, j] * beta[t+1, j]
            xi[t] /= (xi[t].sum() + 1e-10)
        
        # M-step: Update parameters
        # Transition matrix
        for i in range(K):
            for j in range(K):
                transition_matrix[i, j] = xi[:, i, j].sum() / (gamma[:-1, i].sum() + 1e-10)
        
        # Normalize rows
        transition_matrix = transition_matrix / (transition_matrix.sum(axis=1, keepdims=True) + 1e-10)
        
        # Emission parameters
        for k in range(K):
            weights = gamma[:, k]
            total_weight = weights.sum() + 1e-10
            
            means[k] = np.sum(weights * returns) / total_weight
            stds[k] = np.sqrt(np.sum(weights * (returns - means[k])**2) / total_weight)
            stds[k] = max(stds[k], 1e-6)  # Prevent collapse
        
        # Initial probabilities
        initial_probs = gamma[0]
    
    # Final inference
    emission_probs = _gaussian_emission(returns, means, stds)
    alpha, beta, log_likelihood = _forward_backward(
        returns, initial_probs, transition_matrix, emission_probs
    )
    
    gamma = alpha * beta
    gamma = gamma / (gamma.sum(axis=1, keepdims=True) + 1e-10)
    
    # Viterbi decoding
    regime_sequence = _viterbi(initial_probs, transition_matrix, emission_probs)
    
    # Stationary distribution
    eigenvalues, eigenvectors = np.linalg.eig(transition_matrix.T)
    stationary_idx = np.argmax(np.abs(eigenvalues - 1) < 1e-6)
    stationary_dist = np.abs(eigenvectors[:, stationary_idx])
    stationary_dist = stationary_dist / stationary_dist.sum()
    
    # Sort regimes by mean return
    order = np.argsort(means)
    
    return HMMResult(
        n_regimes=n_regimes,
        means=means[order],
        stds=stds[order],
        transition_matrix=transition_matrix[order][:, order],
        stationary_dist=stationary_dist[order],
        regime_sequence=np.array([np.where(order == s)[0][0] for s in regime_sequence]),
        regime_probs=gamma[:, order],
        log_likelihood=log_likelihood
    )


def get_regime_probabilities(
    returns: pd.Series,
    hmm_result: HMMResult
) -> pd.DataFrame:
    """
    Compute regime probabilities for a returns series.
    
    Args:
        returns: Return series
        hmm_result: Fitted HMM result
    
    Returns:
        DataFrame with regime probabilities
    """
    probs = pd.DataFrame(
        hmm_result.regime_probs,
        index=returns.index,
        columns=[f'regime_{i}_prob' for i in range(hmm_result.n_regimes)]
    )
    
    probs['regime'] = hmm_result.regime_sequence
    probs['regime_mean'] = hmm_result.means[hmm_result.regime_sequence]
    probs['regime_std'] = hmm_result.stds[hmm_result.regime_sequence]
    
    return probs


class HMMRegimeDetector:
    """
    Encapsulates HMM-based regime detection.
    
    Example:
        >>> detector = HMMRegimeDetector(n_regimes=3)
        >>> detector.fit(returns_train)
        >>> regime_probs = detector.predict_proba(returns_test)
    """
    
    def __init__(
        self,
        n_regimes: int = 3,
        n_iter: int = 100,
        tol: float = 1e-4,
        random_state: int = 42
    ):
        self.n_regimes = n_regimes
        self.n_iter = n_iter
        self.tol = tol
        self.random_state = random_state
        self.hmm_result_: Optional[HMMResult] = None
        
    def fit(self, returns: pd.Series) -> 'HMMRegimeDetector':
        """Fit HMM to returns."""
        self.hmm_result_ = fit_hmm(
            returns.values,
            n_regimes=self.n_regimes,
            n_iter=self.n_iter,
            tol=self.tol,
            random_state=self.random_state
        )
        return self
    
    def predict(self, returns: pd.Series) -> pd.Series:
        """Predict most likely regime."""
        if self.hmm_result_ is None:
            raise ValueError("Must fit before predict")
        
        # Recompute using fitted parameters
        emission_probs = _gaussian_emission(
            returns.values,
            self.hmm_result_.means,
            self.hmm_result_.stds
        )
        
        # Initial probabilities from stationary
        initial_probs = self.hmm_result_.stationary_dist
        
        regimes = _viterbi(
            initial_probs,
            self.hmm_result_.transition_matrix,
            emission_probs
        )
        
        return pd.Series(regimes, index=returns.index, name='regime')
    
    def predict_proba(self, returns: pd.Series) -> pd.DataFrame:
        """Predict regime probabilities."""
        if self.hmm_result_ is None:
            raise ValueError("Must fit before predict")
        
        emission_probs = _gaussian_emission(
            returns.values,
            self.hmm_result_.means,
            self.hmm_result_.stds
        )
        
        initial_probs = self.hmm_result_.stationary_dist
        
        alpha, beta, _ = _forward_backward(
            returns.values,
            initial_probs,
            self.hmm_result_.transition_matrix,
            emission_probs
        )
        
        gamma = alpha * beta
        gamma = gamma / (gamma.sum(axis=1, keepdims=True) + 1e-10)
        
        return pd.DataFrame(
            gamma,
            index=returns.index,
            columns=[f'regime_{i}_prob' for i in range(self.n_regimes)]
        )
    
    def get_regime_stats(self) -> Dict[str, any]:
        """Get regime statistics."""
        if self.hmm_result_ is None:
            raise ValueError("Must fit first")
        
        return {
            'n_regimes': self.n_regimes,
            'means': self.hmm_result_.means.tolist(),
            'stds': self.hmm_result_.stds.tolist(),
            'stationary_dist': self.hmm_result_.stationary_dist.tolist(),
            'transition_matrix': self.hmm_result_.transition_matrix.tolist(),
        }


def build_regime_features(
    df: pd.DataFrame,
    returns_col: str = 'returns',
    n_regimes: int = 3,
    lookback: int = 252
) -> pd.DataFrame:
    """
    Build regime-based features.
    
    Args:
        df: DataFrame with returns
        returns_col: Returns column name
        n_regimes: Number of HMM regimes
        lookback: Lookback for rolling HMM fit
    
    Returns:
        DataFrame with regime features
    """
    features = pd.DataFrame(index=df.index)
    
    # Fit HMM on full series (or use expanding window in production)
    returns = df[returns_col].dropna()
    
    if len(returns) < 100:
        return features
    
    detector = HMMRegimeDetector(n_regimes=n_regimes)
    detector.fit(returns)
    
    # Get regime probabilities
    regime_probs = detector.predict_proba(returns)
    
    for col in regime_probs.columns:
        features[col] = regime_probs[col]
    
    # Current regime
    features['current_regime'] = detector.predict(returns)
    
    # Regime mean and std
    if detector.hmm_result_ is not None:
        features['regime_mean'] = detector.hmm_result_.means[features['current_regime'].values.astype(int)]
        features['regime_std'] = detector.hmm_result_.stds[features['current_regime'].values.astype(int)]
    
    # Regime change indicator
    features['regime_change'] = (features['current_regime'] != features['current_regime'].shift(1)).astype(int)
    
    # Time since regime change
    regime_changes = features['regime_change'].cumsum()
    features['bars_in_regime'] = features.groupby(regime_changes).cumcount()
    
    # Probability of regime transition
    max_prob = regime_probs.max(axis=1)
    features['regime_uncertainty'] = 1 - max_prob
    
    return features

